# AI Voice Conversation App

Real-time AI voice conversation application using OpenAI Whisper, GPT-4o-mini, and TTS with LiveKit for WebRTC audio streaming.

## Features

- ğŸ¤ **Voice Activity Detection** - Automatically detects when you start and stop speaking
- ğŸ—£ï¸ **Real-time Transcription** - Converts speech to text using OpenAI Whisper
- ğŸ¤– **AI Responses** - Generates intelligent responses using GPT-4o-mini
- ğŸ”Š **Text-to-Speech** - Plays AI responses with high-quality voice synthesis
- â±ï¸ **2-Second Silence Detection** - Automatically sends audio after you finish speaking
- ğŸ”„ **Sequential Flow** - Prevents overlapping conversations for natural interaction

## Tech Stack

**Backend:**
- FastAPI
- OpenAI API (Whisper, GPT-4o-mini, TTS-1-HD)
- LiveKit
- Python 3.11+

**Frontend:**
- Next.js 15
- TypeScript
- LiveKit Client
- Tailwind CSS

## Quick Start

### Prerequisites
- Python 3.11+
- Node.js 18+
- OpenAI API key
- LiveKit credentials (optional, for production)

### 1. Backend Setup

```bash
cd backend
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

Create `backend/.env`:
```
OPENAI_API_KEY=sk-your-key-here
LIVEKIT_API_KEY=your-key
LIVEKIT_API_SECRET=your-secret
LIVEKIT_URL=ws://localhost:7880
```

Run backend:
```bash
uvicorn main:app --reload --port 8000
```

### 2. Frontend Setup

```bash
cd frontend
npm install
```

Create `frontend/.env.local`:
```
NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
```

Run frontend:
```bash
npm run dev
```

Open http://localhost:3000

## Documentation

ğŸ“– **[Backend Documentation](backend/README.md)** - API endpoints, configuration, dependencies

ğŸ“– **[Frontend Documentation](frontend/README.md)** - Components, hooks, architecture

ğŸ“– **[AI Prompts](AI_PROMPTS.md)** - AI tools and prompts used during development

## Project Structure

```
skill.io/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app with endpoints
â”‚   â”œâ”€â”€ ai_handler.py        # OpenAI integrations
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ README.md           # Backend documentation
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/                # Next.js app directory
â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”œâ”€â”€ hooks/              # Custom hooks
â”‚   â””â”€â”€ README.md          # Frontend documentation
â”œâ”€â”€ README.md              # This file
â””â”€â”€ AI_PROMPTS.md         # AI assistance documentation
```

## How It Works

1. **User speaks** â†’ Frontend records audio continuously
2. **2 seconds of silence** â†’ Recording stops, audio sent to backend
3. **Backend transcribes** â†’ OpenAI Whisper converts speech to text
4. **Text displayed** â†’ User sees transcription immediately
5. **AI generates response** â†’ GPT-4o-mini creates reply
6. **Response played** â†’ TTS converts text to speech and plays audio
7. **Ready for next question** â†’ Cycle repeats

## API Endpoints

- `GET /` - Health check
- `POST /token` - Generate LiveKit access token
- `POST /transcribe` - Transcribe audio to text
- `POST /respond` - Generate AI response with audio

## ğŸ¬ Demo

**Live Application Demo:**  
ğŸ”— [Watch Full Demo](https://www.loom.com/share/a63220de061a4379b2888f4006fba01b) - Complete walkthrough of the AI voice conversation app

**Additional Demo:**  
ğŸ”— [Extended Demo](https://www.loom.com/share/213dd54eddc44d8788695b3a4f82306a) - Additional features and functionality